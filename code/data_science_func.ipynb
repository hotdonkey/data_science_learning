{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict, deque\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding outliers by Tukey method with adjustment option for iqr boundaries.\n",
    "\n",
    "def outliers_iqr(data, feature, left_iqr=1.5, right_iqr=1.5, log_scale=False):\n",
    "    \"\"\"Function for finding outliers by Tukey method with adjustment option for iqr's multiplier.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find outliers.\n",
    "        feature (string): Name of a column in DF which will be inspected for outliers.\n",
    "        left_iqr (float, optional): lower boundary multiplier. Defaults to 1.5.\n",
    "        right_iqr (float, optional): upper boundary multiplier. Defaults to 1.5.\n",
    "        log_scale (bool, optional): converting data in logarithmic representation in case of lognormal destribution of\n",
    "        original data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns two copies of the original DataFrame contain DF's with outliers and cleaned data.\n",
    "    \"\"\"    ''''''\n",
    "    \n",
    "    if log_scale:\n",
    "        x = np.log(data[feature])\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    \n",
    "    quartile1, quartile3 = x.quantile(0.25), x.quantile(0.75)\n",
    "    \n",
    "    iqr = quartile3 - quartile1\n",
    "    \n",
    "    lower_bound = quartile1 - (iqr*left_iqr)\n",
    "    \n",
    "    upper_bound = quartile3 + (iqr*right_iqr)\n",
    "    \n",
    "    outliers = data[(x<lower_bound) | (x>upper_bound)]\n",
    "    \n",
    "    cleaned = data[(x>lower_bound) & (x<upper_bound)]\n",
    "    \n",
    "    print(f'Number of outliers by Yukey\\'s method: {outliers.shape[0]}')\n",
    "    print(f'Resulting number of lines cleared of outliers: {cleaned.shape[0]}')\n",
    "    \n",
    "    return outliers, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding outliers by z-method with adjustment option for boundaries.\n",
    "\n",
    "def outliers_z_score(data, feature, left_mod=3, right_mod=3, log_scale=False):\n",
    "    \"\"\"Function for finding outliers by z-method with adjustment option for left and right multiplier.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find outliers.\n",
    "        feature (string): Name of a column in DF which will be inspected for outliers.\n",
    "        left_mod (int, optional): lower boundary multiplier. Defaults to 3.\n",
    "        right_mod (int, optional): upper boundary multiplier. Defaults to 3.\n",
    "        log_scale (bool, optional): converting data in logarithmic representation in case of lognormal destribution of\n",
    "        original data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns two copies of the original DataFrame contain DF's with outliers and cleaned data.\n",
    "    \"\"\"    ''''''\n",
    "    \n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    \n",
    "    mu = x.mean()\n",
    "    \n",
    "    sigma = x.std()\n",
    "    \n",
    "    lower_bound = mu - left_mod * sigma\n",
    "    \n",
    "    upper_bound = mu + right_mod * sigma\n",
    "    \n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    \n",
    "    cleaned = data[(x > lower_bound) & (x < upper_bound)]\n",
    "    \n",
    "    print(f'Number of outliers by z-method: {outliers.shape[0]}')\n",
    "    print(f'Resulting number of lines cleared of outliers: {cleaned.shape[0]}')\n",
    "    \n",
    "    return outliers, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate finding function in lines\n",
    "\n",
    "def dupl_data_remove(data, immune_col=None):\n",
    "    \"\"\"Function for finding full dupliacates in lines\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find duplicates.\n",
    "        immune_col (str or tupple, optional): name of the column/s which the function will pass. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns the copy of the original DataFrame cleaned from full duplicates in line.\n",
    "    \"\"\"    ''''''\n",
    "    \n",
    "    if immune_col is None:                                                          \n",
    "        dupl_columns = list(data.columns)                                           \n",
    "    \n",
    "    else:\n",
    "        dupl_columns = list(data.columns)\n",
    "        try:\n",
    "            for col in immune_col:\n",
    "                dupl_columns.remove(immune_col)  \n",
    "        except ValueError:\n",
    "            pass                                          \n",
    "    \n",
    "    mask = data.duplicated(subset=dupl_columns)                                   \n",
    "    \n",
    "    data_duplicates = data[mask]                                                  \n",
    "    print(f'Number of duplicates: {data_duplicates.shape[0]}')                \n",
    "    \n",
    "    data_dedupped = data.drop_duplicates(subset=dupl_columns)                       \n",
    "    print(f'Resulting number of lines cleared of duplicates: {data_dedupped.shape[0]}')\n",
    "    \n",
    "    return data_dedupped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate finding function in lines (old version)\n",
    "\n",
    "def dupl_data_remove(data, immune_col=None):\n",
    "    \"\"\"Function for finding full dupliacates in lines\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find duplicates.\n",
    "        immune_col (str or tupple, optional): name of the column/s which the function will pass. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns the copy of the original DataFrame cleaned from full duplicates in line.\n",
    "    \"\"\"    ''''''\n",
    "    \n",
    "    if immune_col is None:                                                          \n",
    "        dupl_columns = list(data.columns)                                           \n",
    "    else:\n",
    "        dupl_columns = list(data.columns)                                           \n",
    "        dupl_columns.remove(immune_col)                                             \n",
    "    \n",
    "    mask = data.duplicated(subset=dupl_columns)                                   \n",
    "    \n",
    "    data_duplicates = data[mask]                                                  \n",
    "    print(f'Number of duplicates: {data_duplicates.shape[0]}')                \n",
    "    \n",
    "    data_dedupped = data.drop_duplicates(subset=dupl_columns)                       \n",
    "    print(f'Resulting number of lines cleared of duplicates: {data_dedupped.shape[0]}')\n",
    "    \n",
    "    return data_dedupped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция по поиску не информативных признаков\n",
    "\n",
    "def low_info_col_drop(data, top_freq_thresh=0.95, nuniq_thresh=0.95):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        top_freq_thresh (float, optional): _description_. Defaults to 0.95.\n",
    "        nuniq_thresh (float, optional): _description_. Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    ''''''\n",
    "    \n",
    "    low_info_col_list= []                                                            #\n",
    "    \n",
    "    for col in data.columns:                                                         #\n",
    "        top_freq = data[col].value_counts(normalize=True).max()                      #\n",
    "        nunique_ratio = data[col].nunique() / data[col].count()                      #\n",
    "        \n",
    "    if top_freq > top_freq_thresh:                                                   #\n",
    "        low_info_col_list.append(col)                                                #\n",
    "        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')               #\n",
    "    \n",
    "    if nunique_ratio > nuniq_thresh:                                                 #\n",
    "        low_info_col_list.append(col)                                                #\n",
    "        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')          #\n",
    "        \n",
    "    return low_info_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for cleaning data frame from null data features by conditional level of null data.\n",
    "\n",
    "def null_data_col_drop(data, null_thresh=0.4, immune_col=None):\n",
    "    \"\"\"Function for cleaning the data of empty data features.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used for cleaning.\n",
    "        null_thresh (float, optional): minimal threshold for removing the feature. Defaults to 0.4.\n",
    "        immune_col (str or tupple, optional): name of the column/s which the function will pass. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns the copy of the original DataFrame cleaned from null data columns.\n",
    "    \"\"\"    ''''''\n",
    "    \n",
    "    temp_data = data.replace({0 : np.nan})                         #creating the temp frame with replased 0 by NaN\n",
    "    null_data_col_list = []                                        #empty list for future drop of columns\n",
    "    \n",
    "    for col in temp_data.columns:\n",
    "        try:\n",
    "            col_null = round(temp_data[col].isnull().value_counts(normalize=True), 2)[True]   #percent of NaN in every column\n",
    "        except KeyError:                                                                      #if there is no NaN\n",
    "            col_null = 0\n",
    "        \n",
    "        if col_null > null_thresh:                     #comparison with acceptable level of empty data in column\n",
    "            null_data_col_list.append(col)             \n",
    "            print(f'{col}: {col_null*100}% zero values')\n",
    "    \n",
    "    if immune_col is None:                             #checkin' is there any column/s that the func should skip in drop process\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            for col in immune_col:\n",
    "                null_data_col_list.remove(immune_col)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    drop_data = data.drop(null_data_col_list, axis=1)    \n",
    "    print(f'{drop_data.shape[1]} features with less than {null_thresh*100}% null data')        \n",
    "    \n",
    "    return drop_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b4a17af713f464e4f7b4da81cd20c20d6cf4af2f1712b81a84a456da04343d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
